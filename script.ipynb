{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.utils.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CIFAR 10 Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Size Train Data:  40000\n",
      "Size Valid Data:  10000\n",
      "Size Test Data:  10000\n",
      "Data loaders with augmentation and normalization created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training transformations: Data Augmentation and Normalization\n",
    "#inspo by:https://www.kaggle.com/code/kmldas/cifar10-resnet-90-accuracy-less-than-5-min\n",
    "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "train_transform = transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
    "                         transforms.RandomHorizontalFlip(), \n",
    "                         transforms.ToTensor(), \n",
    "                         transforms.Normalize(*stats,inplace=True)])\n",
    "valid_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*stats)])\n",
    "\n",
    "\n",
    "# Apply transformations\n",
    "CIFAR10_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "CIFAR10_validset = datasets.CIFAR10(root='./data', train=True, download=True, transform=valid_transform)  # Use test_transform for validation\n",
    "CIFAR10_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=valid_transform)\n",
    "\n",
    "# Creating a validation split\n",
    "num_train = len(CIFAR10_trainset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.2 * num_train)) #80%training, 20% validation\n",
    "\n",
    "np.random.seed(42)  # Ensure reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# Assuming you've already defined train_idx and valid_idx for splitting training and validation data\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(CIFAR10_trainset, batch_size=64, sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(CIFAR10_validset, batch_size=64, sampler=valid_sampler)  # No transform argument needed\n",
    "test_loader = torch.utils.data.DataLoader(CIFAR10_testset, batch_size=64, shuffle=False)\n",
    "\n",
    "print('Size Train Data: ', len(train_sampler))\n",
    "print('Size Valid Data: ', len(valid_sampler))\n",
    "print('Size Test Data: ', len(CIFAR10_testset))\n",
    "\n",
    "print(\"Data loaders with augmentation and normalization created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import torchvision\n",
    "\n",
    "# def plot_transformed_images(loader, num_images=10):\n",
    "#     # Ensure we have an even number of images to display\n",
    "#     assert num_images % 2 == 0, \"num_images must be an even number.\"\n",
    "\n",
    "#     # Get a batch of training data\n",
    "#     dataiter = iter(loader)\n",
    "#     images, _ = next(dataiter)\n",
    "    \n",
    "#     # Select a subset of images\n",
    "#     images = images[:num_images]\n",
    "    \n",
    "#     # Use make_grid to create a grid of images\n",
    "#     grid = torchvision.utils.make_grid(images, nrow=num_images//2)\n",
    "\n",
    "#     # Unnormalize and display the images grid\n",
    "#     plt.figure(figsize=(15, 15))\n",
    "#     imshow(grid)\n",
    "\n",
    "# # Update the unnormalize function for a batch of images\n",
    "# def unnormalize(img, mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)):\n",
    "#     img = img.clone()  # Avoid modifying the tensor in-place\n",
    "#     mean = torch.tensor(mean).reshape(3, 1, 1)\n",
    "#     std = torch.tensor(std).reshape(3, 1, 1)\n",
    "#     img.mul_(std).add_(mean)  # Reverse the normalization\n",
    "#     return img\n",
    "\n",
    "# # Update imshow for displaying a grid of images\n",
    "# def imshow(img):\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.axis('off')\n",
    "\n",
    "# # Assuming 'train_loader' is defined and loaded with your specified 'train_transform'\n",
    "# plot_transformed_images(train_loader, num_images=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Constraints: \n",
    "\n",
    "- Resnet\n",
    "- under 5 Mio Params \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "NVIDIA GeForce RTX 4070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        # Reduced the number of layers and planes\n",
    "        self.layer3 = self._make_layer(block, 128, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 256, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(256*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# Adjusting the number of blocks to reduce the total parameters\n",
    "def ResNetModified():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])  # Reduced blocks in each layer\n",
    "\n",
    "def test():\n",
    "    net = ResNetModified()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())\n",
    "\n",
    "test()\n",
    "\n",
    "# Instantiate the model\n",
    "model = ResNetModified()\n",
    "# Move the model to CUDA device if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 3385162\n"
     ]
    }
   ],
   "source": [
    "#Total Parameters in model\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Parameters\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001,weight_decay=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "num_epochs = 40\n",
    "model_path = 'best_model.pth'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "[6337/50000 (16%)]  Loss: 0.2821\n",
      "Epoch: 1\n",
      "[12737/50000 (32%)]  Loss: 0.2743\n",
      "Epoch: 1\n",
      "[19137/50000 (48%)]  Loss: 0.2643\n",
      "Epoch: 1\n",
      "[25537/50000 (64%)]  Loss: 0.2777\n",
      "Epoch: 1\n",
      "[31937/50000 (80%)]  Loss: 0.2788\n",
      "Epoch: 1\n",
      "[38337/50000 (96%)]  Loss: 0.2973\n",
      "\n",
      "Epoch 1: Avg. Training Loss: 0.2785, Avg. Valid Loss: 0.3469, Valid Accuracy: 88.67%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 2\n",
      "[6337/50000 (16%)]  Loss: 0.2521\n",
      "Epoch: 2\n",
      "[12737/50000 (32%)]  Loss: 0.2398\n",
      "Epoch: 2\n",
      "[19137/50000 (48%)]  Loss: 0.2562\n",
      "Epoch: 2\n",
      "[25537/50000 (64%)]  Loss: 0.2611\n",
      "Epoch: 2\n",
      "[31937/50000 (80%)]  Loss: 0.2555\n",
      "Epoch: 2\n",
      "[38337/50000 (96%)]  Loss: 0.2479\n",
      "\n",
      "Epoch 2: Avg. Training Loss: 0.2541, Avg. Valid Loss: 0.3527, Valid Accuracy: 88.17%\n",
      "\n",
      "Epoch: 3\n",
      "[6337/50000 (16%)]  Loss: 0.2234\n",
      "Epoch: 3\n",
      "[12737/50000 (32%)]  Loss: 0.2309\n",
      "Epoch: 3\n",
      "[19137/50000 (48%)]  Loss: 0.2621\n",
      "Epoch: 3\n",
      "[25537/50000 (64%)]  Loss: 0.2434\n",
      "Epoch: 3\n",
      "[31937/50000 (80%)]  Loss: 0.2565\n",
      "Epoch: 3\n",
      "[38337/50000 (96%)]  Loss: 0.2536\n",
      "\n",
      "Epoch 3: Avg. Training Loss: 0.2453, Avg. Valid Loss: 0.3655, Valid Accuracy: 88.36%\n",
      "\n",
      "Epoch: 4\n",
      "[6337/50000 (16%)]  Loss: 0.2098\n",
      "Epoch: 4\n",
      "[12737/50000 (32%)]  Loss: 0.2296\n",
      "Epoch: 4\n",
      "[19137/50000 (48%)]  Loss: 0.2284\n",
      "Epoch: 4\n",
      "[25537/50000 (64%)]  Loss: 0.2379\n",
      "Epoch: 4\n",
      "[31937/50000 (80%)]  Loss: 0.2243\n",
      "Epoch: 4\n",
      "[38337/50000 (96%)]  Loss: 0.2295\n",
      "\n",
      "Epoch 4: Avg. Training Loss: 0.2264, Avg. Valid Loss: 0.3444, Valid Accuracy: 88.87%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 5\n",
      "[6337/50000 (16%)]  Loss: 0.2118\n",
      "Epoch: 5\n",
      "[12737/50000 (32%)]  Loss: 0.2129\n",
      "Epoch: 5\n",
      "[19137/50000 (48%)]  Loss: 0.2151\n",
      "Epoch: 5\n",
      "[25537/50000 (64%)]  Loss: 0.2121\n",
      "Epoch: 5\n",
      "[31937/50000 (80%)]  Loss: 0.2182\n",
      "Epoch: 5\n",
      "[38337/50000 (96%)]  Loss: 0.2223\n",
      "\n",
      "Epoch 5: Avg. Training Loss: 0.2157, Avg. Valid Loss: 0.3378, Valid Accuracy: 89.43%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 6\n",
      "[6337/50000 (16%)]  Loss: 0.1812\n",
      "Epoch: 6\n",
      "[12737/50000 (32%)]  Loss: 0.2124\n",
      "Epoch: 6\n",
      "[19137/50000 (48%)]  Loss: 0.1920\n",
      "Epoch: 6\n",
      "[25537/50000 (64%)]  Loss: 0.2184\n",
      "Epoch: 6\n",
      "[31937/50000 (80%)]  Loss: 0.1964\n",
      "Epoch: 6\n",
      "[38337/50000 (96%)]  Loss: 0.2188\n",
      "\n",
      "Epoch 6: Avg. Training Loss: 0.2030, Avg. Valid Loss: 0.3315, Valid Accuracy: 89.86%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 7\n",
      "[6337/50000 (16%)]  Loss: 0.1688\n",
      "Epoch: 7\n",
      "[12737/50000 (32%)]  Loss: 0.1941\n",
      "Epoch: 7\n",
      "[19137/50000 (48%)]  Loss: 0.1802\n",
      "Epoch: 7\n",
      "[25537/50000 (64%)]  Loss: 0.1963\n",
      "Epoch: 7\n",
      "[31937/50000 (80%)]  Loss: 0.2077\n",
      "Epoch: 7\n",
      "[38337/50000 (96%)]  Loss: 0.2021\n",
      "\n",
      "Epoch 7: Avg. Training Loss: 0.1909, Avg. Valid Loss: 0.3312, Valid Accuracy: 89.70%\n",
      "\n",
      "Epoch: 8\n",
      "[6337/50000 (16%)]  Loss: 0.1687\n",
      "Epoch: 8\n",
      "[12737/50000 (32%)]  Loss: 0.1717\n",
      "Epoch: 8\n",
      "[19137/50000 (48%)]  Loss: 0.1855\n",
      "Epoch: 8\n",
      "[25537/50000 (64%)]  Loss: 0.1855\n",
      "Epoch: 8\n",
      "[31937/50000 (80%)]  Loss: 0.1879\n",
      "Epoch: 8\n",
      "[38337/50000 (96%)]  Loss: 0.1907\n",
      "\n",
      "Epoch 8: Avg. Training Loss: 0.1827, Avg. Valid Loss: 0.3281, Valid Accuracy: 89.74%\n",
      "\n",
      "Epoch: 9\n",
      "[6337/50000 (16%)]  Loss: 0.1505\n",
      "Epoch: 9\n",
      "[12737/50000 (32%)]  Loss: 0.1565\n",
      "Epoch: 9\n",
      "[19137/50000 (48%)]  Loss: 0.1845\n",
      "Epoch: 9\n",
      "[25537/50000 (64%)]  Loss: 0.1746\n",
      "Epoch: 9\n",
      "[31937/50000 (80%)]  Loss: 0.1716\n",
      "Epoch: 9\n",
      "[38337/50000 (96%)]  Loss: 0.1866\n",
      "\n",
      "Epoch 9: Avg. Training Loss: 0.1716, Avg. Valid Loss: 0.3181, Valid Accuracy: 90.04%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 10\n",
      "[6337/50000 (16%)]  Loss: 0.1421\n",
      "Epoch: 10\n",
      "[12737/50000 (32%)]  Loss: 0.1504\n",
      "Epoch: 10\n",
      "[19137/50000 (48%)]  Loss: 0.1655\n",
      "Epoch: 10\n",
      "[25537/50000 (64%)]  Loss: 0.1650\n",
      "Epoch: 10\n",
      "[31937/50000 (80%)]  Loss: 0.1704\n",
      "Epoch: 10\n",
      "[38337/50000 (96%)]  Loss: 0.1630\n",
      "\n",
      "Epoch 10: Avg. Training Loss: 0.1591, Avg. Valid Loss: 0.3402, Valid Accuracy: 89.73%\n",
      "\n",
      "Epoch: 11\n",
      "[6337/50000 (16%)]  Loss: 0.1293\n",
      "Epoch: 11\n",
      "[12737/50000 (32%)]  Loss: 0.1406\n",
      "Epoch: 11\n",
      "[19137/50000 (48%)]  Loss: 0.1576\n",
      "Epoch: 11\n",
      "[25537/50000 (64%)]  Loss: 0.1513\n",
      "Epoch: 11\n",
      "[31937/50000 (80%)]  Loss: 0.1676\n",
      "Epoch: 11\n",
      "[38337/50000 (96%)]  Loss: 0.1554\n",
      "\n",
      "Epoch 11: Avg. Training Loss: 0.1510, Avg. Valid Loss: 0.3459, Valid Accuracy: 89.42%\n",
      "\n",
      "Epoch: 12\n",
      "[6337/50000 (16%)]  Loss: 0.1507\n",
      "Epoch: 12\n",
      "[12737/50000 (32%)]  Loss: 0.1396\n",
      "Epoch: 12\n",
      "[19137/50000 (48%)]  Loss: 0.1542\n",
      "Epoch: 12\n",
      "[25537/50000 (64%)]  Loss: 0.1497\n",
      "Epoch: 12\n",
      "[31937/50000 (80%)]  Loss: 0.1466\n",
      "Epoch: 12\n",
      "[38337/50000 (96%)]  Loss: 0.1608\n",
      "\n",
      "Epoch 12: Avg. Training Loss: 0.1508, Avg. Valid Loss: 0.3275, Valid Accuracy: 90.00%\n",
      "\n",
      "Epoch: 13\n",
      "[6337/50000 (16%)]  Loss: 0.1291\n",
      "Epoch: 13\n",
      "[12737/50000 (32%)]  Loss: 0.1234\n",
      "Epoch: 13\n",
      "[19137/50000 (48%)]  Loss: 0.1556\n",
      "Epoch: 13\n",
      "[25537/50000 (64%)]  Loss: 0.1548\n",
      "Epoch: 13\n",
      "[31937/50000 (80%)]  Loss: 0.1420\n",
      "Epoch: 13\n",
      "[38337/50000 (96%)]  Loss: 0.1562\n",
      "\n",
      "Epoch 13: Avg. Training Loss: 0.1433, Avg. Valid Loss: 0.3325, Valid Accuracy: 90.08%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 14\n",
      "[6337/50000 (16%)]  Loss: 0.1332\n",
      "Epoch: 14\n",
      "[12737/50000 (32%)]  Loss: 0.1180\n",
      "Epoch: 14\n",
      "[19137/50000 (48%)]  Loss: 0.1293\n",
      "Epoch: 14\n",
      "[25537/50000 (64%)]  Loss: 0.1421\n",
      "Epoch: 14\n",
      "[31937/50000 (80%)]  Loss: 0.1382\n",
      "Epoch: 14\n",
      "[38337/50000 (96%)]  Loss: 0.1348\n",
      "\n",
      "Epoch 14: Avg. Training Loss: 0.1331, Avg. Valid Loss: 0.3385, Valid Accuracy: 90.06%\n",
      "\n",
      "Epoch: 15\n",
      "[6337/50000 (16%)]  Loss: 0.1040\n",
      "Epoch: 15\n",
      "[12737/50000 (32%)]  Loss: 0.1159\n",
      "Epoch: 15\n",
      "[19137/50000 (48%)]  Loss: 0.1359\n",
      "Epoch: 15\n",
      "[25537/50000 (64%)]  Loss: 0.1318\n",
      "Epoch: 15\n",
      "[31937/50000 (80%)]  Loss: 0.1274\n",
      "Epoch: 15\n",
      "[38337/50000 (96%)]  Loss: 0.1278\n",
      "\n",
      "Epoch 15: Avg. Training Loss: 0.1244, Avg. Valid Loss: 0.3256, Valid Accuracy: 90.61%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 16\n",
      "[6337/50000 (16%)]  Loss: 0.1095\n",
      "Epoch: 16\n",
      "[12737/50000 (32%)]  Loss: 0.1195\n",
      "Epoch: 16\n",
      "[19137/50000 (48%)]  Loss: 0.1178\n",
      "Epoch: 16\n",
      "[25537/50000 (64%)]  Loss: 0.1227\n",
      "Epoch: 16\n",
      "[31937/50000 (80%)]  Loss: 0.1325\n",
      "Epoch: 16\n",
      "[38337/50000 (96%)]  Loss: 0.1191\n",
      "\n",
      "Epoch 16: Avg. Training Loss: 0.1206, Avg. Valid Loss: 0.3167, Valid Accuracy: 90.80%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 17\n",
      "[6337/50000 (16%)]  Loss: 0.0964\n",
      "Epoch: 17\n",
      "[12737/50000 (32%)]  Loss: 0.0850\n",
      "Epoch: 17\n",
      "[19137/50000 (48%)]  Loss: 0.0696\n",
      "Epoch: 17\n",
      "[25537/50000 (64%)]  Loss: 0.0596\n",
      "Epoch: 17\n",
      "[31937/50000 (80%)]  Loss: 0.0621\n",
      "Epoch: 17\n",
      "[38337/50000 (96%)]  Loss: 0.0692\n",
      "\n",
      "Epoch 17: Avg. Training Loss: 0.0731, Avg. Valid Loss: 0.2674, Valid Accuracy: 92.28%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 18\n",
      "[6337/50000 (16%)]  Loss: 0.0602\n",
      "Epoch: 18\n",
      "[12737/50000 (32%)]  Loss: 0.0545\n",
      "Epoch: 18\n",
      "[19137/50000 (48%)]  Loss: 0.0600\n",
      "Epoch: 18\n",
      "[25537/50000 (64%)]  Loss: 0.0504\n",
      "Epoch: 18\n",
      "[31937/50000 (80%)]  Loss: 0.0495\n",
      "Epoch: 18\n",
      "[38337/50000 (96%)]  Loss: 0.0538\n",
      "\n",
      "Epoch 18: Avg. Training Loss: 0.0547, Avg. Valid Loss: 0.2664, Valid Accuracy: 92.54%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 19\n",
      "[6337/50000 (16%)]  Loss: 0.0517\n",
      "Epoch: 19\n",
      "[12737/50000 (32%)]  Loss: 0.0502\n",
      "Epoch: 19\n",
      "[19137/50000 (48%)]  Loss: 0.0424\n",
      "Epoch: 19\n",
      "[25537/50000 (64%)]  Loss: 0.0472\n",
      "Epoch: 19\n",
      "[31937/50000 (80%)]  Loss: 0.0465\n",
      "Epoch: 19\n",
      "[38337/50000 (96%)]  Loss: 0.0516\n",
      "\n",
      "Epoch 19: Avg. Training Loss: 0.0481, Avg. Valid Loss: 0.2682, Valid Accuracy: 92.64%\n",
      "\n",
      "Saved best model\n",
      "Epoch: 20\n",
      "[6337/50000 (16%)]  Loss: 0.0457\n",
      "Epoch: 20\n",
      "[12737/50000 (32%)]  Loss: 0.0401\n",
      "Epoch: 20\n",
      "[19137/50000 (48%)]  Loss: 0.0431\n",
      "Epoch: 20\n",
      "[25537/50000 (64%)]  Loss: 0.0402\n",
      "Epoch: 20\n",
      "[31937/50000 (80%)]  Loss: 0.0386\n",
      "Epoch: 20\n",
      "[38337/50000 (96%)]  Loss: 0.0457\n",
      "\n",
      "Epoch 20: Avg. Training Loss: 0.0422, Avg. Valid Loss: 0.2768, Valid Accuracy: 92.63%\n",
      "\n",
      "Epoch: 21\n",
      "[6337/50000 (16%)]  Loss: 0.0439\n",
      "Epoch: 21\n",
      "[12737/50000 (32%)]  Loss: 0.0324\n",
      "Epoch: 21\n",
      "[19137/50000 (48%)]  Loss: 0.0365\n",
      "Epoch: 21\n",
      "[25537/50000 (64%)]  Loss: 0.0452\n",
      "Epoch: 21\n",
      "[31937/50000 (80%)]  Loss: 0.0455\n",
      "Epoch: 21\n",
      "[38337/50000 (96%)]  Loss: 0.0375\n",
      "\n",
      "Epoch 21: Avg. Training Loss: 0.0398, Avg. Valid Loss: 0.2749, Valid Accuracy: 92.60%\n",
      "\n",
      "Epoch: 22\n",
      "[6337/50000 (16%)]  Loss: 0.0361\n",
      "Epoch: 22\n",
      "[12737/50000 (32%)]  Loss: 0.0370\n",
      "Epoch: 22\n",
      "[19137/50000 (48%)]  Loss: 0.0332\n",
      "Epoch: 22\n",
      "[25537/50000 (64%)]  Loss: 0.0423\n",
      "Epoch: 22\n",
      "[31937/50000 (80%)]  Loss: 0.0356\n",
      "Epoch: 22\n",
      "[38337/50000 (96%)]  Loss: 0.0403\n",
      "\n",
      "Epoch 22: Avg. Training Loss: 0.0373, Avg. Valid Loss: 0.2813, Valid Accuracy: 92.57%\n",
      "\n",
      "Epoch: 23\n",
      "[6337/50000 (16%)]  Loss: 0.0313\n",
      "Epoch: 23\n",
      "[12737/50000 (32%)]  Loss: 0.0317\n",
      "Epoch: 23\n",
      "[19137/50000 (48%)]  Loss: 0.0298\n",
      "Epoch: 23\n",
      "[25537/50000 (64%)]  Loss: 0.0396\n",
      "Epoch: 23\n",
      "[31937/50000 (80%)]  Loss: 0.0344\n"
     ]
    }
   ],
   "source": [
    "# Resetting histories in case this block is run multiple times\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "valid_accuracy_history = []\n",
    "\n",
    "best_accuracy = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_train_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        total_train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch: {epoch}\\n[{i * len(inputs) + 1}/{len(train_loader.dataset)} ({100. * (i + 1) / len(train_loader):.0f}%)]  Loss: {running_loss / 100:.4f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    scheduler.step()  # Update the learning rate\n",
    "    avg_train_loss = total_train_loss / num_batches\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_valid_loss = 0.0\n",
    "    correct = 0\n",
    "    num_valid_batches = len(valid_loader)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_valid_loss += loss.item()  # Sum the loss of each batch\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / num_valid_batches  #summed up loss divided by number of batches\n",
    "    valid_accuracy = 100. * correct / len(valid_sampler)  # Calculate accuracy\n",
    "\n",
    "    valid_loss_history.append(avg_valid_loss)\n",
    "    valid_accuracy_history.append(valid_accuracy)\n",
    "\n",
    "    print(f'\\nEpoch {epoch}: Avg. Training Loss: {avg_train_loss:.4f}, Avg. Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\\n')\n",
    "    \n",
    "    # Save the model if it has the best accuracy so far\n",
    "    if valid_accuracy > best_accuracy:\n",
    "        best_accuracy = valid_accuracy\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"Saved best model\")\n",
    "\n",
    "print(f'Execution time: {time.time() - start_time:.2f} seconds')\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss_history, '-', linewidth=3, label='Train Loss')\n",
    "plt.plot(valid_loss_history, '-', linewidth=3, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87.81%\n"
     ]
    }
   ],
   "source": [
    "#load best model\n",
    "#requires init of model. \n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.to(device)  # Move model to the appropriate device (CPU or GPU)\n",
    "\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Variables to track correct predictions and total predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# No gradient is needed for evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the same device as the model\n",
    "        inputs, labels = inputs.to(device), labels.to(device) #test data to device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get the predicted classes\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Update total and correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
